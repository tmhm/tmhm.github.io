<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on Well</title>
    <link>https://tmhm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on Well</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-EN</language>
    <managingEditor>wells217@163.com (Well)</managingEditor>
    <webMaster>wells217@163.com (Well)</webMaster>
    <copyright>(c) 2017 Well.</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 15:26:00 +0000</lastBuildDate>
    <atom:link href="https://tmhm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>统计学习笔记（0）</title>
      <link>https://tmhm.github.io/2016/04/20/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B00/</link>
      <pubDate>Wed, 20 Apr 2016 15:26:00 +0000</pubDate>
      <author>wells217@163.com (Well)</author>
      <guid>https://tmhm.github.io/2016/04/20/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B00/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;监督学习 （supervised learning）&lt;/li&gt;
&lt;li&gt;非监督学习 （unsupervised learning）&lt;/li&gt;
&lt;li&gt;半监督学习 （semi-supervised learning）&lt;/li&gt;
&lt;li&gt;强化学习 （reinforcement learning）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;一直以为强化学习不属于统计学习的范畴，看来过去臆想了。&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;监督学习基本概念&#34;&gt;监督学习基本概念&lt;/h3&gt;

&lt;h4 id=&#34;输入-特征-输出空间&#34;&gt;输入、特征、输出空间&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;输入与输出的所有值分别称之为&lt;strong&gt;输入空间&lt;/strong&gt;和&lt;strong&gt;输出空间&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;所有特征向量存在的空间称为&lt;strong&gt;特征空间&lt;/strong&gt;，特征空间的每一维对应于一个特征；&lt;/li&gt;
&lt;li&gt;有时，输入空间会和特征空间一致；有时会不同，输入空间往往会经过某些变换将输入空间映射到特征空间；&lt;/li&gt;
&lt;li&gt;模型实际是都是定义在特征空间上的；&lt;/li&gt;
&lt;li&gt;人们根据输入和输出变量的不同类型来*区分不同的预测任务*：

&lt;ul&gt;
&lt;li&gt;输入和输出均为连续变量的预测问题称之为&lt;strong&gt;回归问题&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;输出为有限个离散变量的预测问题称之为&lt;strong&gt;分类问题&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;输入和输出均为变量序列的预测问题称之为&lt;strong&gt;标注问题&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;联合概率分布&#34;&gt;联合概率分布&lt;/h4&gt;

&lt;p&gt;监督学习假设输入和输出遵循联合概率分布$P(X,Y)$.&lt;/p&gt;

&lt;h4 id=&#34;假设空间&#34;&gt;假设空间&lt;/h4&gt;

&lt;p&gt;监督学习的目的在于学习一个有输入到输出的映射，这一映射由模型来表示。
模型属于由输入空间到输出空间的映射的集合，这一集合就是&lt;strong&gt;假设空间（hypothesis space）&lt;/strong&gt;，假设空间的确定意味着学习范围的确定。&lt;/p&gt;

&lt;p&gt;监督学习的模型可以是概率模型或非概率模型，用条件概率分布或决策函数表示。&lt;/p&gt;

&lt;h4 id=&#34;问题形式化&#34;&gt;问题形式化&lt;/h4&gt;

&lt;p&gt;监督学习分学习和预测两个过程。学习过程是利用训练数据集学习一个模型，再用学习到的模型对测试样本进行预测，即预测过程。&lt;/p&gt;

&lt;p&gt;一个具体的模型$y=f(x)$,对一个输入$x_i$,可以产生一个输出$f(x_i)$,而训练模型中对应的输出是$y_i$,如果这个模型训练的足够好，有很好的预测能力，则其训练样本的输出$y_i$和模型的输出$f(x_i)$之间的差就应该足够小。学习系统就是通过不断尝试，选取最好的模型，以便对训练数据集具有最好的预测，同时对未知的测试数据集的预测也有尽可能好的推广，即泛化能力。&lt;/p&gt;

&lt;h3 id=&#34;统计学习三要素&#34;&gt;统计学习三要素&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;模型

&lt;ul&gt;
&lt;li&gt;模型的假设空间包含所有可能的条件概率或决策函数&lt;/li&gt;
&lt;li&gt;参数空间&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;策略
（学习的准则）&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;损失函数和风险函数&lt;br /&gt;

&lt;ul&gt;
&lt;li&gt;0-1、 平方、 绝对、 对数损失函数&lt;/li&gt;
&lt;li&gt;损失函数的期望，即平均意义下的损失，称之为风险函数或期望损失&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;经验风险最小化与结构风险最小化&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;经验最小化的策略认为经验最小的模型是最优的，当样本数量很小时会出现“过拟合”&lt;/li&gt;
&lt;li&gt;结构最小化是为了防止“过拟合”提出，其等价于正则化。结构风险在经验风险上加上表示模型复杂度的正则项或者惩罚项&lt;/li&gt;
&lt;li&gt;$\frac{1}{N} \sum_{i=1}^{N} L(y_i,f(x_i)) +\lambda \ast J(f)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;$J(f)$为模型的复杂度，模型越复杂，复杂度越大，即复杂度表示了对复杂模型的惩罚，$\lambda$系数 大于0， 用以权衡经验风险和模型复杂度。结构风险小需要经验风险与模型复杂度同时小。&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;算法&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;统计学习问题归结为最优化问题&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;模型的评估与模型选择&#34;&gt;模型的评估与模型选择&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;一般考虑到&lt;strong&gt;训练误差&lt;/strong&gt;和&lt;strong&gt;测试误差&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;避免过拟合&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;正则化与交叉验证&#34;&gt;正则化与交叉验证&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;正则项一般有1-范数和2-范数&lt;/li&gt;

&lt;li&gt;&lt;p&gt;交叉验证&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;简单交叉验证，即随机将已知数据分2部分，分别作为训练和测试，然后将训练集在各种参数条件下训练，最后在测试集上评估，选出测试误差最小的模型；&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;S折交叉验证，首先随机将数据切分为S个互不相交的大小相同的子集，然后用S-1个子集用于训练，余下的作为测试，重复选择S次，最后选择S次测试中平均测试误差最小的模型&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;留一交叉验证，S折交叉验证的特殊情形S=N，N为给定数据集的容量，即每次只有一个数据样本用于测试。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;泛化能力&#34;&gt;泛化能力&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;即模型对未知数据的预测能力。&lt;/li&gt;
&lt;li&gt;理论上可以通过泛化误差上界的大小来进行分析。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;生成模型与判别模型&#34;&gt;生成模型与判别模型&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;根据采用的方式是生成方法和判别方法而来。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;生成方法&lt;/strong&gt;是由数据学习联合概率分布，然后求得条件概率分布作为模型
    * 典型有：朴素贝叶斯法和隐马尔科夫模型&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;判别方法&lt;/strong&gt;是由数据直接学习决策函数或者条件概率分布作为模型
    * 典型有：K近邻，感知机，决策树，逻辑回归，最大熵模型，SVM，提升方法（AdaBoost），条件随机场等&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;区别：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;生成方法可以还原出联合概率分布，判别则不行；&lt;/li&gt;
&lt;li&gt;生成方法学习的收敛速度更快&lt;/li&gt;
&lt;li&gt;当存在隐变量，仍可以用生成方法，而此时判别方法行不通&lt;/li&gt;
&lt;li&gt;判别方法直接学习条件概率或决策函数，直接预测，往往学习的准确率更高；由于直接学习，可以对数据进行各种程度上的抽象、定义特征并使用特征，也可以简化学习问题&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;面向问题&#34;&gt;面向问题&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;分类问题&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;指标:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;TP&amp;ndash;将正类分为正的；&lt;/li&gt;
&lt;li&gt;FN&amp;ndash;将正类分为负的；&lt;/li&gt;
&lt;li&gt;FP&amp;ndash;将负类分为正的；&lt;/li&gt;

&lt;li&gt;&lt;p&gt;TN&amp;ndash;将负类分为负的。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;精确率 P = TP/(TP+FP)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;召回率 R = TP/(TP+FN)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;以上两者的调和均值 2/F = 1/P + 1/R&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;标注问题&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;回归问题&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一元回归和多元回归&lt;/li&gt;
&lt;li&gt;线性和非线性&lt;/li&gt;
&lt;li&gt;常用损失函数&amp;ndash;平方损失函数&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;非监督学习&#34;&gt;非监督学习&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;数据没有类别信息，也不给定目标值&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&#34;典型代表&#34;&gt;典型代表：&lt;/h6&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;聚类&lt;/strong&gt;将数据集合分成由类似的对象组成的多个类&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;密度估计&lt;/strong&gt;用于寻找数据统计值&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;降维&lt;/strong&gt;，用于展示数据或者预处理&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;e.g&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;K-均值&lt;/li&gt;
&lt;li&gt;最大期望算法&lt;/li&gt;
&lt;li&gt;DBSCAN(Density-based spatial clustering of applications with noise)&lt;/li&gt;
&lt;li&gt;Parzen窗设计&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;参考：&lt;/p&gt;

&lt;p&gt;统计学习方法，李航
机器学习实战， Peter Harrington&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>流形学习笔记</title>
      <link>https://tmhm.github.io/2015/09/18/%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Fri, 18 Sep 2015 14:50:00 +0000</pubDate>
      <author>wells217@163.com (Well)</author>
      <guid>https://tmhm.github.io/2015/09/18/%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</guid>
      <description>&lt;p&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;维数约简&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;特征选择，依据某一标准选择性质最突出的特征&lt;/li&gt;
&lt;li&gt;特征抽取，经已有特征的某种变换获取约简特征&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;增加特征数：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;可以增加信息量，进而提高准确度&lt;/li&gt;
&lt;li&gt;增加训练分类器的难度，进而带来维数灾难。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;解决办法：&lt;/p&gt;
&lt;p&gt;　　选取尽可能多的、可能有用的特征，然后根据需要进行特征约简。&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;主成分分析（PCA）&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong style=&#34;line-height: 1.5;&#34;&gt;&lt;img style=&#34;display: block; margin-left: auto; margin-right: auto;&#34; src=&#34;http://images2015.cnblogs.com/blog/781469/201509/781469-20150918214617414-906516153.png&#34; alt=&#34;&#34; /&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;目的：&lt;/p&gt;
&lt;p&gt;　　寻找能够表示采样数据的最好投影子空间。&lt;/p&gt;
&lt;p&gt;求解：&lt;/p&gt;
&lt;p&gt;　　对样本的散布矩阵（scatter matrix）进行特征值分解，所求之空间为过样本均值，（何为过样本均值？）&lt;/p&gt;
&lt;p&gt;以最大特征值所对应的特征向量为方向的之空间。&lt;/p&gt;
&lt;p&gt;特点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对椭球状分布的样本集具有很好的效果，理论上，学习到的主方向就是椭圆的主轴方向。&lt;/li&gt;
&lt;li&gt;非监督学习算法，能找到很好地代表所有样本的方向，但是，对于分类未必是最有利的，如下图：　　&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img style=&#34;display: block; margin-left: auto; margin-right: auto;&#34; src=&#34;http://images2015.cnblogs.com/blog/781469/201509/781469-20150918215131414-1106255874.png&#34; alt=&#34;&#34; width=&#34;451&#34; height=&#34;260&#34; /&gt;&lt;/p&gt;
&lt;p&gt;线性判别分析（LDA）&lt;/p&gt;
&lt;p&gt;思想：&lt;/p&gt;
&lt;p&gt;　　寻找最能把两类样本分开的投影直线&lt;/p&gt;
&lt;p&gt;特点：&lt;/p&gt;
&lt;p&gt;　　监督的维数约简&lt;/p&gt;
&lt;p&gt;目标：&lt;/p&gt;
&lt;p&gt;　　是投影后两类样本的均值之差与投影样本的总类散步的比值最大。&lt;/p&gt;
&lt;p&gt;&amp;lt;img style=&amp;laquo;display: block; margin-left: auto; margin-right: auto;&amp;raquo; src=&amp;laquo;&lt;a href=&#34;http://images2015.cnblogs.com/blog/781469/201509/781469-20150918215637336-153&#34;&gt;http://images2015.cnblogs.com/blog/781469/201509/781469-20150918215637336-153&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;求解：&lt;/p&gt;

&lt;p&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;多重判别分析（MDA）&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;　　LDA往多类情况的推广&lt;/p&gt;
&lt;p&gt;　　解法与LDA类似，对于C－类问题，把样本投影到C－１维之空间。&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;线性方法的缺点：&lt;/p&gt;
&lt;p&gt;线性方法对于某些数据不能进行有效的处理，这类数据，在现实中，往往很多。比如下图：&lt;/p&gt;
&lt;p&gt;&lt;img style=&#34;display: block; margin-left: auto; margin-right: auto;&#34; src=&#34;http://images2015.cnblogs.com/blog/781469/201509/781469-20150918220506773-237605601.jpg&#34; alt=&#34;&#34; width=&#34;389&#34; height=&#34;292&#34; /&gt;&lt;/p&gt;

&lt;p&gt;另外，现实中的数据往往并不是特征的线性组合。&lt;/p&gt;
&lt;p&gt;比如　paper：A Global Geometric Framework　for Nonlinear Dimensionality　Reduction　所提到的人脸模型&lt;/p&gt;

&lt;p&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;流形学习&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;是线性之空间的一种非线性推广；&lt;/li&gt;
&lt;li&gt;一种局部可坐标话的拓扑空间结构；&lt;/li&gt;
&lt;li&gt;一种非线性额维数约简方法。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;基本思想：&lt;/p&gt;
&lt;p&gt;　　高维观测空间中的点是由少数独立变量的共同作用在观测空间张成一个流形，如果能有效地展开观测空间卷曲的流形或者发现其内在的主要变量，就可以对该数据集进行降维。&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;可行性分析：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;许多高维采样数据都是由少数几个隐含变量所决定的, 如人脸采样由光线亮度, 人离相机的距离, 人的头部姿势, 人的脸部肌肉等因素决定；&lt;/li&gt;
&lt;li&gt;从认知心理学的角度来看, 心理学家认为人的认知过程是基于认知流形和拓扑连续性的。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;几种经典的流形学习方法：&lt;/p&gt;
&lt;p&gt;　　局部线性嵌入（LLE）　Nonlinear dimensionality reduction by locally linear embedding. Science&lt;/p&gt;
&lt;p&gt;　　等距映射（IsoMap）A global geometric framework for nonlinear dimensionality reduction. Science,&lt;/p&gt;
&lt;p&gt;　　拉普拉斯特征映射（Laplacian　Eigenmap）Laplacian Eigenmaps for Dimensionality Reduction and Data Representation. Neural Computation,&lt;/p&gt;

&lt;p&gt;&lt;p&gt;参考：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;自动化所　流形学习　课件。&lt;/li&gt;
&lt;li&gt;paper：A Global Geometric Framework　for Nonlinear Dimensionality　Reduction，　ｓｃｉｅｎｃｅ，２０００&lt;/li&gt;
&lt;/ol&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>